{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import namedtuple\n",
    "from tensorflow.python.keras.models import clone_model, load_model, Model\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Input, concatenate\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.initializers import Zeros, Ones, Constant, he_normal, truncated_normal\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Import custom classes\n",
    "# from Classes.generators import generatorWienerProcess, generatorGeometricBM\n",
    "# from Classes.callOptionBlackScholes import callOption\n",
    "from Classes.callOptionBlackScholes import call_option\n",
    "from Classes.generators import geometric_brownian_generator, wiener_process_generator\n",
    "\n",
    "# Reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brains, Agents, Environments, and Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "# Initialize the Replay Buffer class\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []                # a circular queue\n",
    "        self.index = 0\n",
    "    \n",
    "    # push a new tuple to the buffer\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "        self.memory[self.index] = Transition(state, action, next_state, reward)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        \n",
    "    # sample a random batch for training\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    # return the buffer length\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDelta:\n",
    "    def __init__(self, num_states, num_actions, is_rounded = True):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.is_rounded = is_rounded\n",
    "    \n",
    "    def replay(self):\n",
    "        return\n",
    "    \n",
    "    def decide_action(self, state, episode=None):\n",
    "        if self.is_rounded:\n",
    "            return np.round(state[3])\n",
    "        else:\n",
    "            return state[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDQN:\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        \n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.freq_update_target = FREQ_UPDATE_TARGET\n",
    "        self.count_replay = 0\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "        \n",
    "        # Construct a neural network\n",
    "        input_dim = self.num_states\n",
    "        output_dim = self.num_actions\n",
    "        hidden_layers = [16, 16,]\n",
    "        activation_func = 'relu'\n",
    "        k_init = he_normal(seed=0)\n",
    "        optimizer = Adam(lr=0.001)\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        for i, fan_in in enumerate( ([input_dim,]+hidden_layers)[:-1] ):\n",
    "            fan_out = hidden_layers[i]\n",
    "            self.model.add(Dense(input_dim=fan_in, units=fan_out, kernel_initializer=k_init))\n",
    "            # self.model.add(BatchNormalization())\n",
    "            self.model.add(Activation(activation_func))\n",
    "        self.model.add(Dense(units=output_dim, kernel_initializer=truncated_normal(seed=0)))\n",
    "        \n",
    "        # print(self.model)\n",
    "        \n",
    "        # Set how to train the model\n",
    "        self.model.compile(loss='mse', optimizer=optimizer,)\n",
    "        self._target_model = clone_model(self.model)\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        self.model.save(file_name)\n",
    "    \n",
    "    def load(self, file_name):\n",
    "        self.model = load_model(file_name)\n",
    "        self._target_model = clone_model(self.model)\n",
    "    \n",
    "    def replay(self):\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE: return\n",
    "        \n",
    "        # Make mini batch\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        transitions = list(zip(*transitions))\n",
    "        \n",
    "        states = np.array(transitions[0])\n",
    "        actions = np.array(transitions[1])\n",
    "        n_states = np.array(transitions[2])\n",
    "        rewards = np.array(transitions[3])\n",
    "        \n",
    "        X = np.zeros_like(states)\n",
    "        for i, n_s in enumerate(n_states): \n",
    "            if n_s is not None:\n",
    "                X[i, :] = n_s\n",
    "            else:\n",
    "                X[i, :] = states[i, :]\n",
    "        n_q = np.max(self._target_model.predict(X), axis=1)\n",
    "        \n",
    "        # Make targets for regression\n",
    "        q = self.model.predict(states)\n",
    "        for i, n_s in enumerate(n_states): \n",
    "            r = rewards[i]\n",
    "            a = actions[i]\n",
    "            if n_s is not None:\n",
    "                r += GAMMA*n_q[i]\n",
    "            q[i,a] = r\n",
    "        \n",
    "        # Update weight parameters\n",
    "        loss = self.model.train_on_batch(states, q)\n",
    "        \n",
    "        self.count_replay += 1\n",
    "        if self.count_replay % self.freq_update_target == 0:\n",
    "            self.update_target_model()\n",
    "    \n",
    "    def decide_action(self, state, episode=None):\n",
    "        \n",
    "        if episode is not None:\n",
    "            epsilon = 0.5 * (1 / (episode + 1))\n",
    "        else:\n",
    "            epsilon = 0\n",
    "        \n",
    "        p = np.random.uniform(0,1)\n",
    "        if p >= epsilon:\n",
    "            state = np.array(state).reshape(1,-1)\n",
    "            action = np.argmax(self.model.predict(state)[0])\n",
    "        else:\n",
    "            action = np.random.randint(0, self.num_actions)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self._target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is implemented referring to the below:\n",
    "# https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "class Actor:\n",
    "    \n",
    "    def __init__(self, env_dim, act_dim, act_range, lr, tau):\n",
    "        self.env_dim = env_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.act_range = act_range\n",
    "        self.tau = tau\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Make models and optimizer\n",
    "        self.model = self.network()\n",
    "        self.target_model = self.network()\n",
    "        self.my_optimizer = self.optimizer()\n",
    "    \n",
    "    def network(self):\n",
    "        inp = Input(shape=(self.env_dim,))\n",
    "        x = Dense(12, activation='relu', kernel_initializer=he_normal(seed=0))(inp)\n",
    "        x = Dense(12, activation='relu', kernel_initializer=he_normal(seed=0))(x)\n",
    "        x = Dense(12, activation='relu', kernel_initializer=he_normal(seed=0))(x)\n",
    "        out = Dense(self.act_dim, activation='linear', kernel_initializer=truncated_normal(seed=0))(x)\n",
    "        return Model(inp, out)\n",
    "    \n",
    "    def make_action(self, state):\n",
    "        # the input is a state vector [shape = (self.env_dim,)]\n",
    "        action = self.model.predict(np.expand_dims(state, axis=0)) # shape (self.env_dim,) --> (1, self.env_dim)\n",
    "        return action\n",
    "    \n",
    "    def target_predict(self, states):\n",
    "        # the input is state vectors [shape = (batch_size, self.env_dim)]\n",
    "        return self.target_model.predict(states)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1.0 - self.tau) * target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    \n",
    "    def train(self, states, actions, grads):\n",
    "        # grads = dQ / da_i\n",
    "        # calculate the total grads [i.e., sum_i (-1) *(dQ/da_i) * (da_i / dw_j)] and apply them to the actor weights w_j.\n",
    "        self.my_optimizer([states, grads])\n",
    "    \n",
    "    def optimizer(self):\n",
    "        action_gdts = backend.placeholder(shape=(None, self.act_dim)) # = dQ/da_i\n",
    "        params_grad = tf.gradients(self.model.output, self.model.trainable_weights, -action_gdts) # = sum_i (-1) *(dQ/da_i) * (da_i / dw_j)\n",
    "        grads = zip(params_grad, self.model.trainable_weights)\n",
    "        return backend.function([self.model.input, action_gdts], outputs=[], updates=[tf.train.GradientDescentOptimizer(self.lr).apply_gradients(grads),])\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.save(path + '_actor.h5')\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model = load_model(path + '_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is implemented referring to the below:\n",
    "# https://github.com/germain-hug/Deep-RL-Keras/tree/master/DDPG\n",
    "class Critic:\n",
    "    \n",
    "    def __init__(self, env_dim, act_dim, lr, tau):\n",
    "        self.env_dim = env_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.tau = tau\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Make models\n",
    "        self.model = self.network()\n",
    "        self.target_model = self.network()\n",
    "        self.model.compile(Adam(self.lr), 'mse')\n",
    "        self.target_model.compile(Adam(self.lr), 'mse')\n",
    "        \n",
    "        # Function to compute Q-value gradients wrt action (= dQ/da_i)\n",
    "        # input[0]: state, input[1]: action\n",
    "        self.action_grads = backend.function([self.model.input[0], self.model.input[1]], [backend.gradients(self.model.output, [self.model.input[1],]),])\n",
    "    \n",
    "    def network(self):\n",
    "        state = Input(shape=(self.env_dim,))\n",
    "        action = Input(shape=(self.act_dim,))\n",
    "        x = concatenate([state, action])\n",
    "        x = Dense(24, activation='relu', kernel_initializer=he_normal(seed=0))(x)\n",
    "        x = Dense(24, activation='relu', kernel_initializer=he_normal(seed=0))(x)\n",
    "        x = Dense(24, activation='relu', kernel_initializer=he_normal(seed=0))(x)\n",
    "        out = Dense(1, activation='linear', kernel_initializer=truncated_normal(seed=0))(x)\n",
    "        return Model([state, action], out) # = Q(s,a)\n",
    "    \n",
    "    def gradients(self, states, actions):\n",
    "        return self.action_grads([states, actions]) # = dQ/da_i\n",
    "    \n",
    "    def target_predict(self, inputs):\n",
    "        # inputs are [states, actions]\n",
    "        return self.target_model.predict(inputs)\n",
    "    \n",
    "    def train(self, states, actions, critic_targets):\n",
    "        return self.model.train_on_batch([states, actions], critic_targets)\n",
    "    \n",
    "    def transfer_weights(self):\n",
    "        W, target_W = self.model.get_weights(), self.target_model.get_weights()\n",
    "        for i in range(len(W)):\n",
    "            target_W[i] = self.tau * W[i] + (1.0 - self.tau)* target_W[i]\n",
    "        self.target_model.set_weights(target_W)\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.save(path + '_critic.h5')\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model = load_model(path + '_critic.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentActionValueBase:\n",
    "    def __init__(self, brain):\n",
    "        self.brain = brain\n",
    "    \n",
    "    def update_agent(self):\n",
    "        self.brain.replay()\n",
    "    \n",
    "    def get_action(self, state, step=None, episode=None):\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below three classes are copied from the below:\n",
    "# https://github.com/keras-rl/keras-rl/blob/master/rl/random.py\n",
    "\n",
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    \n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = np.random.normal(self.mu,self.current_sigma,self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDDPG:\n",
    "    \n",
    "    def __init__(self, env_dim, act_dim, act_range, random_process=None, actor_lr=1e-3, critic_lr=1e-3, tau=1e-3):\n",
    "        self.env_dim = env_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.act_range = act_range\n",
    "        \n",
    "        self.episode = 0\n",
    "        self.random_process =  random_process\n",
    "        if random_process is not None:\n",
    "            self.random_process.reset_states()\n",
    "        \n",
    "        # Make models and memory\n",
    "        self.actor = Actor(self.env_dim, self.act_dim, self.act_range, actor_lr, tau)\n",
    "        self.critic = Critic(self.env_dim, self.act_dim, critic_lr, tau)\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "    \n",
    "    def get_action(self, s, step=None, episode=None):\n",
    "        \n",
    "        if self.random_process is None:\n",
    "            \n",
    "            if episode is not None:\n",
    "                epsilon = 0.5 * (1 / (episode + 1))\n",
    "            else:\n",
    "                epsilon = 0\n",
    "            \n",
    "            p = np.random.uniform(0,1)\n",
    "            if p >= epsilon:\n",
    "                action = np.round(self.actor.make_action(s)[0,0])\n",
    "                action = np.clip(action, self.act_range[0], self.act_range[1])\n",
    "            else:\n",
    "                action = np.random.randint(self.act_range[0], self.act_range[1]+1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            noise = 0\n",
    "            if episode is not None:\n",
    "                noise = self.random_process.sample()\n",
    "            \n",
    "            action = np.round(self.actor.make_action(s)[0,0]+noise)\n",
    "            action = np.clip(action, self.act_range[0], self.act_range[1])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.memory.push(state, action, state_next, reward)\n",
    "    \n",
    "    def update_agent(self):\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE: return\n",
    "        \n",
    "        # Make mini batch\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        transitions = list(zip(*transitions))\n",
    "        \n",
    "        states = np.array(transitions[0])\n",
    "        actions = np.array(transitions[1])\n",
    "        n_states = np.array(transitions[2])\n",
    "        rewards = np.array(transitions[3])\n",
    "        \n",
    "        X = np.zeros_like(states)\n",
    "        for i, n_s in enumerate(n_states): \n",
    "            if n_s is not None:\n",
    "                X[i, :] = n_s\n",
    "        \n",
    "        # Make critic targets using the target networks\n",
    "        critic_targets = rewards\n",
    "        n_q = self.critic.target_predict([X, self.actor.target_predict(X)])\n",
    "        for i, n_s in enumerate(n_states): \n",
    "            if n_s is not None:\n",
    "                critic_targets[i] += GAMMA * n_q[i]\n",
    "        \n",
    "        # Train critic\n",
    "        self.critic.train(states, actions, critic_targets)\n",
    "        \n",
    "        # Q-value gradients under the current policy\n",
    "        actions = self.actor.model.predict(states)\n",
    "        grads = self.critic.gradients(states, actions) # = dQ/da_i\n",
    "        \n",
    "        # Train actor\n",
    "        grads = np.array(grads).reshape((-1, self.act_dim)) # = dQ/da_i\n",
    "        self.actor.train(states, actions, grads)\n",
    "        \n",
    "        # Transfer weights to the target networks at rate tau\n",
    "        self.actor.transfer_weights()\n",
    "        self.critic.transfer_weights()\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.actor.save(path)\n",
    "        self.critic.save(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.critic.load(path)\n",
    "        self.actor.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentBS():\n",
    "    \n",
    "    def __init__(self, T, n_steps, mu, ir, vol, S0, K, num_sold_opt, kappa, alpha=0):\n",
    "        self.T = T\n",
    "        self.n_steps = n_steps\n",
    "        self.mu = mu\n",
    "        self.ir = ir\n",
    "        self.vol = vol\n",
    "        self.S0 = S0\n",
    "        self.K = K\n",
    "        self.num_sold_opt = num_sold_opt\n",
    "        self.kappa = kappa\n",
    "        self.alpha = alpha\n",
    "        self.seed = 0\n",
    "        \n",
    "        self.dt = T/n_steps\n",
    "        self.num_states = 5\n",
    "        self.num_actions = self.num_sold_opt + 1\n",
    "        self.initialize_paths = False\n",
    "    \n",
    "    def __generate_paths(self, n_paths=10000):\n",
    "        \n",
    "        self.n_paths = n_paths\n",
    "        self.idx_path = -1\n",
    "        \n",
    "        # Make stock paths\n",
    "        S_paths, W_paths = geometric_brownian_generator().generate_S_and_W(self.dt, self.n_paths, self.n_steps+1, self.mu, self.vol, self.S0, self.seed)\n",
    "        \n",
    "        # Make time paths\n",
    "        t_paths = np.zeros_like(S_paths)\n",
    "        for j in range(S_paths.shape[1]): t_paths[:, j] = self.dt * j\n",
    "        \n",
    "        # Make option prices and deltas paths\n",
    "        call_opt_paths = call_option.calc_prices(S_paths, self.K, self.ir, self.vol, self.T)\n",
    "        delta_paths = call_option.calc_deltas(S_paths, self.K, self.ir, self.vol, self.T)\n",
    "        \n",
    "        self.S_paths = S_paths\n",
    "        self.W_paths = W_paths\n",
    "        self.t_paths = t_paths\n",
    "        self.call_opt_paths = call_opt_paths\n",
    "        self.delta_paths = delta_paths\n",
    "    \n",
    "    def __get_state_without_num_stocks(self, i_path, j_time):\n",
    "        \n",
    "        t = self.t_paths[i_path, j_time]\n",
    "        W = self.W_paths[i_path, j_time]\n",
    "        C = self.call_opt_paths[i_path, j_time]\n",
    "        delta = self.delta_paths[i_path, j_time] * self.num_sold_opt # multiplied by num options\n",
    "        num_stk = 0\n",
    "        \n",
    "        return np.array([t, W, C, delta, num_stk]) # state\n",
    "    \n",
    "    def __get_cost(self, S, chg_nS):\n",
    "        return self.alpha * S * (np.abs(chg_nS) + 0.01 * chg_nS**2)\n",
    "    \n",
    "    def clear_all_paths(self):\n",
    "        self.initialize_paths = False\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        if not self.initialize_paths:\n",
    "            self.__generate_paths()\n",
    "            self.initialize_paths = True\n",
    "        \n",
    "        self.idx_path = (self.idx_path + 1) % self.n_paths\n",
    "        self.idx_time = 0\n",
    "        \n",
    "        state = self.__get_state_without_num_stocks(self.idx_path, self.idx_time)\n",
    "        self.state = state\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        if self.idx_time > self.n_steps:\n",
    "            n_state = None\n",
    "            r = np.nan\n",
    "            done = True\n",
    "            info = None\n",
    "            \n",
    "        elif self.idx_time == self.n_steps:\n",
    "            n_state = None\n",
    "            r = self.__get_reward(n_state)\n",
    "            done = True\n",
    "            info = self.__get_pv(n_state)\n",
    "            \n",
    "        else:\n",
    "            self.idx_time += 1\n",
    "            n_state = self.__get_state_without_num_stocks(self.idx_path, self.idx_time)\n",
    "            n_state[4] = action # num of stocks is updated.\n",
    "            r = self.__get_reward(n_state)\n",
    "            done = False\n",
    "            info = self.__get_pv(n_state)\n",
    "        \n",
    "        self.state = n_state\n",
    "        \n",
    "        return n_state, r, done, info\n",
    "    \n",
    "    def __get_reward(self, n_state=None):\n",
    "        \n",
    "        if n_state is None or self.state[0] == self.T:\n",
    "            r = self.num_sold_opt * (np.exp(self.ir * self.T) - 1) * self.call_opt_paths[self.idx_path, 0]\n",
    "            r = r / np.power(GAMMA, self.T)\n",
    "            return r\n",
    "        \n",
    "        t1 = n_state[0]\n",
    "        t0 = self.state[0]\n",
    "        \n",
    "        S1 =  self.S0 * np.exp(self.vol * n_state[1] + (self.mu - self.vol**2 / 2) * t1)\n",
    "        S0 = self.S0 * np.exp(self.vol * self.state[1] + (self.mu - self.vol**2 / 2) * t0)\n",
    "        \n",
    "        C1 = n_state[2]\n",
    "        C0 = self.state[2]\n",
    "        \n",
    "        d1 = n_state[3] # = delta per one option * num sold options\n",
    "        d0 = self.state[3] # = delta per one option * num sold options\n",
    "        \n",
    "        nS1 = n_state[4]\n",
    "        nS0 = self.state[4]\n",
    "        \n",
    "        r = nS1 * S1 - nS0 * S0\n",
    "        r -= self.num_sold_opt * (C1 - C0)\n",
    "        r -= (nS1 - nS0) * S0 * np.exp(self.ir * (self.T - t0))\n",
    "        \n",
    "        if self.alpha > 0:\n",
    "            cost = self.__get_cost(S=S0, chg_nS=(nS1 - nS0))\n",
    "            r -= cost * np.exp(self.ir * (self.T - t0))\n",
    "        \n",
    "        if self.kappa > 0:\n",
    "            var = self.vol * S0 * (nS1 - d0)\n",
    "            var = var**2 * self.dt\n",
    "            r -= self.kappa * var / 2\n",
    "        \n",
    "        r = r / np.power(GAMMA, t0)\n",
    "        \n",
    "        return r\n",
    "    \n",
    "    def __get_pv(self, n_state=None):\n",
    "        \n",
    "        if n_state is None or self.state[0] == self.T:\n",
    "            r = self.num_sold_opt * (np.exp(self.ir * self.T) - 1) * self.call_opt_paths[self.idx_path, 0]\n",
    "            return r\n",
    "        \n",
    "        t1 = n_state[0]\n",
    "        t0 = self.state[0]\n",
    "        \n",
    "        S1 =  self.S0 * np.exp(self.vol * n_state[1] + (self.mu - self.vol**2 / 2) * t1)\n",
    "        S0 = self.S0 * np.exp(self.vol * self.state[1] + (self.mu - self.vol**2 / 2) * t0)\n",
    "        \n",
    "        C1 = n_state[2]\n",
    "        C0 = self.state[2]\n",
    "        \n",
    "        d1 = n_state[3] # = delta per one option * num sold options\n",
    "        d0 = self.state[3] # = delta per one option * num sold options\n",
    "        \n",
    "        nS1 = n_state[4]\n",
    "        nS0 = self.state[4]\n",
    "        \n",
    "        r = nS1 * S1 - nS0 * S0\n",
    "        r -= self.num_sold_opt * (C1 - C0)\n",
    "        r -= (nS1 - nS0) * S0 * np.exp(self.ir * (self.T - t0))\n",
    "        \n",
    "        if self.alpha > 0:\n",
    "            cost = self.__get_cost(S=S0, chg_nS=(nS1 - nS0))\n",
    "            r -= cost * np.exp(self.ir * (self.T - t0))\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerBS:\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "    \n",
    "    def run(self, num_episodes, num_steps, seed = 999):\n",
    "        \n",
    "        hists = None\n",
    "        self.env.clear_all_paths()\n",
    "        self.env.seed = seed\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            if hists is None:\n",
    "                hists = np.zeros(shape=(num_episodes, num_steps+1, len(state)+1))\n",
    "            \n",
    "            for step in range(num_steps+1):\n",
    "                action = self.agent.get_action(state, None, None)\n",
    "                n_state, reward, done, pv = self.env.step(action)\n",
    "                \n",
    "                hists[episode, step, :-1] = state\n",
    "                hists[episode, step, -1] = pv\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = n_state\n",
    "        \n",
    "        return hists\n",
    "    \n",
    "    def train(self, num_episodes, num_steps, seed = 0, num_pl_to_calc_std = 30):\n",
    "        \n",
    "        self.env.seed = seed\n",
    "        lst_pl = []\n",
    "        lst_std_pl = []\n",
    "        lst_r = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            state = self.env.reset()\n",
    "            pl = []\n",
    "            sum_r = 0\n",
    "            \n",
    "            for step in range(num_steps+1):\n",
    "                action = self.agent.get_action(state, step=step, episode=episode)\n",
    "                n_state, reward, done, pv = self.env.step(action)\n",
    "                sum_r += reward * np.power(GAMMA, step)\n",
    "                \n",
    "                pl.append(pv)\n",
    "                self.agent.memorize(state, action, n_state, reward)\n",
    "                self.agent.update_agent()\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = n_state\n",
    "            \n",
    "            lst_r.append(sum_r)\n",
    "            pl = sum(pl)\n",
    "            if len(lst_pl) < num_pl_to_calc_std:\n",
    "                lst_pl.append(pl)\n",
    "                lst_std_pl.append(np.nan)\n",
    "            else:\n",
    "                i = episode % num_pl_to_calc_std\n",
    "                lst_pl[i] = pl\n",
    "                std = statistics.stdev(lst_pl)\n",
    "                lst_std_pl.append(std)\n",
    "                if episode%20 == 0: print(\"Episode {}:  P&L Std = {}\".format(episode, np.round(std, 2)))\n",
    "        \n",
    "        return lst_std_pl, lst_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Hedging Agent\n",
    "\n",
    "## Delta (No transaction costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9g/fzy2mr7x0m37g03y40g40s3c0000gn/T/ipykernel_6428/3175206262.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerBS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magentDelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mhists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9g/fzy2mr7x0m37g03y40g40s3c0000gn/T/ipykernel_6428/1874502889.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_episodes, num_steps, seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mhists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9g/fzy2mr7x0m37g03y40g40s3c0000gn/T/ipykernel_6428/2429721756.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_time\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mn_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_pv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9g/fzy2mr7x0m37g03y40g40s3c0000gn/T/ipykernel_6428/2429721756.py\u001b[0m in \u001b[0;36m__get_reward\u001b[0;34m(self, GAMMA, n_state)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sold_opt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mir\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_opt_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "NUM_STEPS = 30 * 1\n",
    "MU = 0.05 / 365\n",
    "IR = MU\n",
    "VOL = 0.20 / np.sqrt(365)\n",
    "S0 = 100\n",
    "K = 100\n",
    "NUM_SOLD_OPT = 100\n",
    "KAPPA = 0.1\n",
    "ALPHA = 0\n",
    "GAMMA = 0.99\n",
    "\n",
    "np.random.seed(999)\n",
    "random.seed(999)\n",
    "\n",
    "for NUM_STEPS in [T*1, T*2, T*4, T*8]:\n",
    "    \n",
    "    env = EnvironmentBS(T, NUM_STEPS, MU, IR, VOL, S0, K, NUM_SOLD_OPT, KAPPA, ALPHA)\n",
    "    num_states = env.num_states\n",
    "    num_actions = env.num_actions\n",
    "    \n",
    "    brainDelta = BrainDelta(num_states, num_actions, is_rounded=True)\n",
    "    agentDelta = AgentActionValueBase(brainDelta)\n",
    "    \n",
    "    trainer = TrainerBS(env, agentDelta)\n",
    "    \n",
    "    hists = trainer.run(2000, NUM_STEPS)\n",
    "    pl = np.sum(hists[:, :, -1], axis=1)\n",
    "    \n",
    "    print('n_steps per day = {0:.0f}, AvePL = {1:.2f}, StdPL = {2:.2f}'.format(NUM_STEPS/T, pl.mean(), pl.std(ddof=1)))\n",
    "    sns.distplot(pl, label=\"nsteps/day = {}\".format(int(NUM_STEPS/T)))\n",
    "    #sns.histplot(pl, label=\"nsteps/day = {}\".format(int(NUM_STEPS/T)), kde=True, stat=\"density\", linewidth=0)\n",
    "\n",
    "plt.xlim(-100,100)\n",
    "plt.axvline(0, color='k', ls='--')\n",
    "plt.xlabel(\"Profit and Loss\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(\"Case of No Costs: Delta\")\n",
    "plt.legend(frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f93ea3d1f85819981c6085c5f035c9a2c3862a717a956ec8a64bcbd28b7074fd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('deep-hedging-RL-MkYESUZK': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
